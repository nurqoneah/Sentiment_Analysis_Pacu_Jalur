{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Based Sentiment Analysis with IndoBERT\n",
    "\n",
    "This notebook uses the new SentimentAnalyzer class with:\n",
    "- External data files for slang, emoji, and stopwords\n",
    "- IndoBERT transformer model for sentiment analysis\n",
    "- LLM prompting approach as fallback\n",
    "- Advanced preprocessing and duplicate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path to import sentiment_analyzer\n",
    "sys.path.append('..')\n",
    "from sentiment_analyzer import SentimentAnalyzer\n",
    "\n",
    "# Set style for matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SentimentAnalyzer...\n",
      "Loaded 263 slang mappings\n",
      "Loaded 678 emoji mappings\n",
      "Loaded 753 stopwords\n",
      "Initializing IndoBERT model...\n",
      "Trying to load model: indobenchmark/indobert-base-p1\n",
      "Failed to load indobenchmark/indobert-base-p1: There was a specific connection error when trying to load indobenchmark/indobert-base-p1:\n",
      "401 Client Error: Unauthorized for url: https://huggingface.co/indobenchmark/indobert-base-p1/resolve/main/config.json (Request ID: Root=1-68a7ca6a-2661d7e543641d6158e5e566;f626a2ef-6606-4c32-afba-7d8c28d3888d)\n",
      "\n",
      "Invalid credentials in Authorization header\n",
      "Trying to load model: cahya/bert-base-indonesian-522M\n",
      "Failed to load cahya/bert-base-indonesian-522M: There was a specific connection error when trying to load cahya/bert-base-indonesian-522M:\n",
      "401 Client Error: Unauthorized for url: https://huggingface.co/cahya/bert-base-indonesian-522M/resolve/main/config.json (Request ID: Root=1-68a7ca6a-0b2ec3dd1d5caf8b5f0cc636;f7c4a6ac-9dcd-4786-80dd-d6d9ea4bff17)\n",
      "\n",
      "Invalid credentials in Authorization header\n",
      "Trying to load model: indolem/indobert-base-uncased\n",
      "Failed to load indolem/indobert-base-uncased: There was a specific connection error when trying to load indolem/indobert-base-uncased:\n",
      "401 Client Error: Unauthorized for url: https://huggingface.co/indolem/indobert-base-uncased/resolve/main/config.json (Request ID: Root=1-68a7ca6b-2d5815300085f4a759bb2eff;e769f4d4-838c-4ab8-96c9-9acc241ba507)\n",
      "\n",
      "Invalid credentials in Authorization header\n",
      "Failed to load IndoBERT models, using fallback English model...\n",
      "Error initializing model: There was a specific connection error when trying to load cardiffnlp/twitter-roberta-base-sentiment-latest:\n",
      "401 Client Error: Unauthorized for url: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/config.json (Request ID: Root=1-68a7ca6b-2518731d1a32dacd320d9e14;8d5e330d-bbca-4e2d-ad14-9dacc01b6197)\n",
      "\n",
      "Invalid credentials in Authorization header\n",
      "Using LLM prompting approach as fallback...\n",
      "\n",
      "Model Information:\n",
      "  has_transformer: False\n",
      "  slang_mappings: 263\n",
      "  emoji_mappings: 678\n",
      "  stopwords_count: 753\n",
      "  device: cpu\n",
      "\n",
      "Sentiment analyzer ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sentiment analyzer\n",
    "print(\"Initializing SentimentAnalyzer...\")\n",
    "analyzer = SentimentAnalyzer(data_dir=\"../data\")\n",
    "\n",
    "# Get model information\n",
    "model_info = analyzer.get_model_info()\n",
    "print(\"\\nModel Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nSentiment analyzer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TikTok data not found\n",
      "Instagram data not found\n",
      "No data found. Please run the scrapers first.\n"
     ]
    }
   ],
   "source": [
    "def load_and_clean_data():\n",
    "    \"\"\"Load data from both platforms and remove duplicates\"\"\"\n",
    "    data_frames = []\n",
    "    \n",
    "    # Load TikTok data\n",
    "    tiktok_path = '../data/tiktok/all_tiktok_comments.csv'\n",
    "    if os.path.exists(tiktok_path):\n",
    "        tiktok_df = pd.read_csv(tiktok_path)\n",
    "        tiktok_df['platform'] = 'TikTok'\n",
    "        tiktok_df['text'] = tiktok_df.get('comment', '')\n",
    "        tiktok_df['post_id'] = tiktok_df.get('aweme_id', '')\n",
    "        tiktok_df['user_id'] = tiktok_df.get('username', '')\n",
    "        data_frames.append(tiktok_df)\n",
    "        print(f\"Loaded {len(tiktok_df)} TikTok comments\")\n",
    "    else:\n",
    "        print(\"TikTok data not found\")\n",
    "    \n",
    "    # Load Instagram data\n",
    "    instagram_path = '../data/instagram/all_comments.csv'\n",
    "    if os.path.exists(instagram_path):\n",
    "        instagram_df = pd.read_csv(instagram_path)\n",
    "        instagram_df['platform'] = 'Instagram'\n",
    "        instagram_df['text'] = instagram_df.get('comment_text', '')\n",
    "        instagram_df['post_id'] = instagram_df.get('post_id', '')\n",
    "        instagram_df['user_id'] = instagram_df.get('username', '')\n",
    "        data_frames.append(instagram_df)\n",
    "        print(f\"Loaded {len(instagram_df)} Instagram comments\")\n",
    "    else:\n",
    "        print(\"Instagram data not found\")\n",
    "    \n",
    "    if not data_frames:\n",
    "        print(\"No data found. Please run the scrapers first.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine dataframes\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    print(f\"\\nTotal comments before cleaning: {len(combined_df)}\")\n",
    "    \n",
    "    # Remove empty comments\n",
    "    combined_df = combined_df[combined_df['text'].notna()]\n",
    "    combined_df = combined_df[combined_df['text'].str.strip() != '']\n",
    "    print(f\"After removing empty comments: {len(combined_df)}\")\n",
    "    \n",
    "    # Remove duplicates based on username and post_id (potential spam)\n",
    "    print(f\"\\nDuplicate detection:\")\n",
    "    duplicates = combined_df.duplicated(subset=['user_id', 'post_id'], keep='first')\n",
    "    print(f\"Found {duplicates.sum()} potential spam comments (same user, same post)\")\n",
    "    \n",
    "    # Show some examples of duplicates before removing\n",
    "    if duplicates.sum() > 0:\n",
    "        print(\"\\nExamples of potential spam:\")\n",
    "        spam_examples = combined_df[duplicates][['user_id', 'post_id', 'text', 'platform']].head()\n",
    "        print(spam_examples)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    combined_df = combined_df[~duplicates]\n",
    "    print(f\"\\nAfter removing duplicates: {len(combined_df)}\")\n",
    "    \n",
    "    # Also remove exact text duplicates (copy-paste spam)\n",
    "    text_duplicates = combined_df.duplicated(subset=['text'], keep='first')\n",
    "    print(f\"Found {text_duplicates.sum()} exact text duplicates\")\n",
    "    combined_df = combined_df[~text_duplicates]\n",
    "    print(f\"After removing text duplicates: {len(combined_df)}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Load the data\n",
    "df = load_and_clean_data()\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"\\nFinal dataset:\")\n",
    "    print(f\"Total comments: {len(df)}\")\n",
    "    print(f\"Platforms: {df['platform'].value_counts().to_dict()}\")\n",
    "    print(f\"Unique users: {df['user_id'].nunique()}\")\n",
    "    print(f\"Unique posts: {df['post_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"Applying advanced preprocessing...\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    df['cleaned_text'] = df['text'].apply(analyzer.preprocess_text)\n",
    "    \n",
    "    # Remove comments that became empty after cleaning\n",
    "    df = df[df['cleaned_text'].str.len() > 0]\n",
    "    print(f\"After advanced cleaning: {len(df)} comments\")\n",
    "    \n",
    "    # Show examples of preprocessing\n",
    "    print(\"\\nPreprocessing examples:\")\n",
    "    examples = df[['text', 'cleaned_text']].head(10)\n",
    "    for idx, row in examples.iterrows():\n",
    "        print(f\"Original: {row['text'][:100]}...\")\n",
    "        print(f\"Cleaned:  {row['cleaned_text'][:100]}...\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Also create version without stopwords for word clouds\n",
    "    df['cleaned_no_stopwords'] = df['cleaned_text'].apply(\n",
    "        lambda x: analyzer.preprocess_text(x, remove_stopwords=True)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(f\"Original text - Mean: {df['text'].str.len().mean():.1f}, Median: {df['text'].str.len().median():.1f}\")\n",
    "    print(f\"Cleaned text - Mean: {df['cleaned_text'].str.len().mean():.1f}, Median: {df['cleaned_text'].str.len().median():.1f}\")\n",
    "    print(f\"No stopwords - Mean: {df['cleaned_no_stopwords'].str.len().mean():.1f}, Median: {df['cleaned_no_stopwords'].str.len().median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM-Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"Performing LLM-based sentiment analysis...\")\n",
    "    \n",
    "    # Analyze sentiment using the new analyzer\n",
    "    sentiment_results = []\n",
    "    \n",
    "    for idx, text in enumerate(df['cleaned_text']):\n",
    "        if idx % 10 == 0:  # Progress indicator\n",
    "            print(f\"Processing {idx}/{len(df)} comments...\")\n",
    "        \n",
    "        result = analyzer.analyze_sentiment(text)\n",
    "        sentiment_results.append(result)\n",
    "    \n",
    "    # Extract results into separate columns\n",
    "    df['sentiment'] = [r['sentiment'] for r in sentiment_results]\n",
    "    df['confidence'] = [r['confidence'] for r in sentiment_results]\n",
    "    df['method'] = [r.get('method', 'unknown') for r in sentiment_results]\n",
    "    \n",
    "    print(\"\\nSentiment analysis completed!\")\n",
    "    \n",
    "    print(f\"\\nSentiment distribution:\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "    print(f\"\\nMethod distribution:\")\n",
    "    print(df['method'].value_counts())\n",
    "    \n",
    "    print(f\"\\nAverage confidence: {df['confidence'].mean():.3f}\")\n",
    "    print(f\"Confidence by sentiment:\")\n",
    "    print(df.groupby('sentiment')['confidence'].agg(['mean', 'std', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Sentiment distribution\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    axes[0, 0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Sentiment Distribution (LLM-based)')\n",
    "    \n",
    "    # 2. Platform-wise sentiment\n",
    "    platform_sentiment = pd.crosstab(df['platform'], df['sentiment'])\n",
    "    platform_sentiment.plot(kind='bar', ax=axes[0, 1], rot=45)\n",
    "    axes[0, 1].set_title('Sentiment Distribution by Platform')\n",
    "    axes[0, 1].legend(title='Sentiment')\n",
    "    \n",
    "    # 3. Confidence distribution\n",
    "    axes[0, 2].hist(df['confidence'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 2].axvline(df['confidence'].mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {df[\"confidence\"].mean():.3f}')\n",
    "    axes[0, 2].set_title('Confidence Distribution')\n",
    "    axes[0, 2].set_xlabel('Confidence Score')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # 4. Method distribution\n",
    "    method_counts = df['method'].value_counts()\n",
    "    axes[1, 0].bar(method_counts.index, method_counts.values)\n",
    "    axes[1, 0].set_title('Analysis Method Distribution')\n",
    "    axes[1, 0].set_xlabel('Method')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Confidence by sentiment\n",
    "    df.boxplot(column='confidence', by='sentiment', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Confidence by Sentiment')\n",
    "    axes[1, 1].set_xlabel('Sentiment')\n",
    "    axes[1, 1].set_ylabel('Confidence')\n",
    "    \n",
    "    # 6. Text length vs confidence\n",
    "    df['text_length'] = df['cleaned_text'].str.len()\n",
    "    axes[1, 2].scatter(df['text_length'], df['confidence'], alpha=0.6)\n",
    "    axes[1, 2].set_title('Text Length vs Confidence')\n",
    "    axes[1, 2].set_xlabel('Text Length (characters)')\n",
    "    axes[1, 2].set_ylabel('Confidence')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(text_data, title=\"Word Cloud\", max_words=100):\n",
    "    \"\"\"Create and display word cloud\"\"\"\n",
    "    if not text_data:\n",
    "        print(\"No text data available for word cloud\")\n",
    "        return\n",
    "    \n",
    "    # Combine all text\n",
    "    all_text = ' '.join(text_data)\n",
    "    \n",
    "    if not all_text.strip():\n",
    "        print(f\"No meaningful text for {title}\")\n",
    "        return\n",
    "    \n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        background_color='white',\n",
    "        max_words=max_words,\n",
    "        colormap='viridis'\n",
    "    ).generate(all_text)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if not df.empty:\n",
    "    # Overall word cloud (using text without stopwords)\n",
    "    create_wordcloud(df['cleaned_no_stopwords'].tolist(), \"Overall Comments Word Cloud\")\n",
    "    \n",
    "    # Word clouds by sentiment\n",
    "    for sentiment in ['positif', 'negatif', 'netral']:\n",
    "        sentiment_texts = df[df['sentiment'] == sentiment]['cleaned_no_stopwords'].tolist()\n",
    "        if sentiment_texts:\n",
    "            create_wordcloud(sentiment_texts, f\"{sentiment.title()} Comments Word Cloud\")\n",
    "    \n",
    "    # Word clouds by platform\n",
    "    for platform in df['platform'].unique():\n",
    "        platform_texts = df[df['platform'] == platform]['cleaned_no_stopwords'].tolist()\n",
    "        if platform_texts:\n",
    "            create_wordcloud(platform_texts, f\"{platform} Comments Word Cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Save the processed dataset\n",
    "    output_path = '../data/llm_sentiment_analysis_results.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Processed data saved to: {output_path}\")\n",
    "    \n",
    "    # Create comprehensive summary\n",
    "    summary_stats = {\n",
    "        'total_comments': len(df),\n",
    "        'platform_distribution': df['platform'].value_counts().to_dict(),\n",
    "        'sentiment_distribution': df['sentiment'].value_counts().to_dict(),\n",
    "        'method_distribution': df['method'].value_counts().to_dict(),\n",
    "        'average_confidence': float(df['confidence'].mean()),\n",
    "        'confidence_by_sentiment': df.groupby('sentiment')['confidence'].mean().to_dict(),\n",
    "        'model_info': analyzer.get_model_info(),\n",
    "        'preprocessing_stats': {\n",
    "            'original_avg_length': float(df['text'].str.len().mean()),\n",
    "            'cleaned_avg_length': float(df['cleaned_text'].str.len().mean()),\n",
    "            'no_stopwords_avg_length': float(df['cleaned_no_stopwords'].str.len().mean())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('../data/llm_model_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary_stats, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"Model summary saved for Flask app\")\n",
    "    \n",
    "    # Display final statistics\n",
    "    print(\"\\n=== LLM SENTIMENT ANALYSIS SUMMARY ===\")\n",
    "    print(f\"Total comments analyzed: {len(df)}\")\n",
    "    print(f\"\\nSentiment Distribution:\")\n",
    "    for sentiment, count in df['sentiment'].value_counts().items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {sentiment}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nMethod Distribution:\")\n",
    "    for method, count in df['method'].value_counts().items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {method}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nAverage Confidence: {df['confidence'].mean():.3f}\")\n",
    "    print(f\"\\nModel Information:\")\n",
    "    for key, value in analyzer.get_model_info().items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nLLM-based sentiment analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
