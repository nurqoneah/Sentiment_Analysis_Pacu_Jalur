{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Sentiment Analysis for Social Media Comments\n",
    "\n",
    "This notebook performs comprehensive sentiment analysis with advanced preprocessing:\n",
    "- Emoji to text conversion\n",
    "- Slang normalization\n",
    "- Stopword removal\n",
    "- Duplicate detection and removal\n",
    "- Word cloud generation\n",
    "- Sentiment distribution visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Set style for matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indonesian slang dictionary\n",
    "SLANG_DICT = {\n",
    "    'gw': 'saya', 'gue': 'saya', 'w': 'saya', 'aku': 'saya',\n",
    "    'lu': 'kamu', 'lo': 'kamu', 'u': 'kamu', 'km': 'kamu',\n",
    "    'bgt': 'banget', 'bgt': 'banget', 'bgtt': 'banget',\n",
    "    'yg': 'yang', 'yng': 'yang', 'yank': 'yang',\n",
    "    'dgn': 'dengan', 'dg': 'dengan', 'sm': 'sama',\n",
    "    'tp': 'tapi', 'tpi': 'tapi', 'tp': 'tetapi',\n",
    "    'ga': 'tidak', 'gak': 'tidak', 'g': 'tidak', 'tdk': 'tidak',\n",
    "    'udh': 'sudah', 'udah': 'sudah', 'dah': 'sudah',\n",
    "    'blm': 'belum', 'blom': 'belum',\n",
    "    'krn': 'karena', 'krna': 'karena', 'soalnya': 'karena',\n",
    "    'jd': 'jadi', 'jdi': 'jadi', 'jadinya': 'jadi',\n",
    "    'kl': 'kalau', 'klo': 'kalau', 'kalo': 'kalau',\n",
    "    'gmn': 'gimana', 'gmna': 'gimana', 'bagaimana': 'gimana',\n",
    "    'knp': 'kenapa', 'knpa': 'kenapa', 'mengapa': 'kenapa',\n",
    "    'emg': 'memang', 'emang': 'memang', 'mmg': 'memang',\n",
    "    'bkn': 'bukan', 'bukn': 'bukan',\n",
    "    'hrs': 'harus', 'hrus': 'harus', 'mesti': 'harus',\n",
    "    'bs': 'bisa', 'bsa': 'bisa', 'dapat': 'bisa',\n",
    "    'org': 'orang', 'orng': 'orang', 'people': 'orang',\n",
    "    'skrg': 'sekarang', 'skrang': 'sekarang', 'now': 'sekarang',\n",
    "    'ntr': 'nanti', 'nanti': 'nanti', 'later': 'nanti',\n",
    "    'kmrn': 'kemarin', 'kemaren': 'kemarin', 'yesterday': 'kemarin',\n",
    "    'bsok': 'besok', 'besok': 'besok', 'tomorrow': 'besok',\n",
    "    'mantap': 'bagus', 'mantul': 'bagus', 'keren': 'bagus',\n",
    "    'jelek': 'buruk', 'ancur': 'buruk', 'parah': 'buruk',\n",
    "    'wkwk': 'haha', 'wkwkwk': 'haha', 'hehe': 'haha',\n",
    "    'anjay': 'wow', 'anjir': 'wow', 'astaga': 'wow',\n",
    "    'bro': 'saudara', 'sis': 'saudara', 'guys': 'teman'\n",
    "}\n",
    "\n",
    "# Emoji to text mapping (common ones)\n",
    "EMOJI_DICT = {\n",
    "    '😀': 'senang', '😃': 'senang', '😄': 'senang', '😁': 'senang',\n",
    "    '😆': 'tertawa', '😅': 'tertawa', '🤣': 'tertawa', '😂': 'tertawa',\n",
    "    '🙂': 'senyum', '😊': 'senyum', '😇': 'senyum',\n",
    "    '😍': 'cinta', '🥰': 'cinta', '😘': 'cinta', '💕': 'cinta', '❤️': 'cinta',\n",
    "    '😢': 'sedih', '😭': 'sedih', '😿': 'sedih', '💔': 'sedih',\n",
    "    '😠': 'marah', '😡': 'marah', '🤬': 'marah', '💢': 'marah',\n",
    "    '😱': 'kaget', '😨': 'takut', '😰': 'cemas',\n",
    "    '🤔': 'bingung', '😕': 'bingung', '😐': 'biasa',\n",
    "    '👍': 'bagus', '👎': 'jelek', '👏': 'tepuk tangan',\n",
    "    '🔥': 'keren', '💯': 'sempurna', '✨': 'bagus',\n",
    "    '🙏': 'terima kasih', '💪': 'kuat', '👌': 'oke'\n",
    "}\n",
    "\n",
    "def convert_emojis_to_text(text):\n",
    "    \"\"\"Convert emojis to meaningful text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    for emoji, meaning in EMOJI_DICT.items():\n",
    "        text = text.replace(emoji, f' {meaning} ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_slang(text):\n",
    "    \"\"\"Normalize Indonesian slang words\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    words = text.split()\n",
    "    normalized_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Remove punctuation from word for lookup\n",
    "        clean_word = word.strip(string.punctuation)\n",
    "        if clean_word in SLANG_DICT:\n",
    "            normalized_words.append(SLANG_DICT[clean_word])\n",
    "        else:\n",
    "            normalized_words.append(word)\n",
    "    \n",
    "    return ' '.join(normalized_words)\n",
    "\n",
    "def remove_stopwords(text, language='indonesian'):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        stop_words = set(stopwords.words(language))\n",
    "    except:\n",
    "        # If Indonesian not available, use English\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add common Indonesian stopwords\n",
    "    indonesian_stopwords = {\n",
    "        'dan', 'atau', 'tetapi', 'namun', 'karena', 'sebab', 'oleh', 'untuk',\n",
    "        'dari', 'ke', 'di', 'pada', 'dalam', 'dengan', 'tanpa', 'seperti',\n",
    "        'ini', 'itu', 'tersebut', 'yang', 'adalah', 'akan', 'telah', 'sudah',\n",
    "        'belum', 'masih', 'juga', 'hanya', 'saja', 'pun', 'lah', 'kah'\n",
    "    }\n",
    "    stop_words.update(indonesian_stopwords)\n",
    "    \n",
    "    words = word_tokenize(str(text).lower())\n",
    "    filtered_words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def advanced_clean_text(text):\n",
    "    \"\"\"Comprehensive text cleaning function\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Convert emojis to text\n",
    "    text = convert_emojis_to_text(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove numbers (optional - you might want to keep some)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra punctuation but keep sentence structure\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)\n",
    "    \n",
    "    # Normalize slang\n",
    "    text = normalize_slang(text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Preprocessing functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Duplicate Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data():\n",
    "    \"\"\"Load data from both platforms and remove duplicates\"\"\"\n",
    "    data_frames = []\n",
    "    \n",
    "    # Load TikTok data\n",
    "    tiktok_path = '../data/tiktok/all_tiktok_comments.csv'\n",
    "    if os.path.exists(tiktok_path):\n",
    "        tiktok_df = pd.read_csv(tiktok_path)\n",
    "        tiktok_df['platform'] = 'TikTok'\n",
    "        tiktok_df['text'] = tiktok_df.get('comment', '')\n",
    "        tiktok_df['post_id'] = tiktok_df.get('aweme_id', '')\n",
    "        tiktok_df['user_id'] = tiktok_df.get('username', '')\n",
    "        data_frames.append(tiktok_df)\n",
    "        print(f\"Loaded {len(tiktok_df)} TikTok comments\")\n",
    "    else:\n",
    "        print(\"TikTok data not found\")\n",
    "    \n",
    "    # Load Instagram data\n",
    "    instagram_path = '../data/instagram/all_comments.csv'\n",
    "    if os.path.exists(instagram_path):\n",
    "        instagram_df = pd.read_csv(instagram_path)\n",
    "        instagram_df['platform'] = 'Instagram'\n",
    "        instagram_df['text'] = instagram_df.get('comment_text', '')\n",
    "        instagram_df['post_id'] = instagram_df.get('post_id', '')\n",
    "        instagram_df['user_id'] = instagram_df.get('username', '')\n",
    "        data_frames.append(instagram_df)\n",
    "        print(f\"Loaded {len(instagram_df)} Instagram comments\")\n",
    "    else:\n",
    "        print(\"Instagram data not found\")\n",
    "    \n",
    "    if not data_frames:\n",
    "        print(\"No data found. Please run the scrapers first.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine dataframes\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    print(f\"\\nTotal comments before cleaning: {len(combined_df)}\")\n",
    "    \n",
    "    # Remove empty comments\n",
    "    combined_df = combined_df[combined_df['text'].notna()]\n",
    "    combined_df = combined_df[combined_df['text'].str.strip() != '']\n",
    "    print(f\"After removing empty comments: {len(combined_df)}\")\n",
    "    \n",
    "    # Remove duplicates based on username and post_id (potential spam)\n",
    "    print(f\"\\nDuplicate detection:\")\n",
    "    duplicates = combined_df.duplicated(subset=['user_id', 'post_id'], keep='first')\n",
    "    print(f\"Found {duplicates.sum()} potential spam comments (same user, same post)\")\n",
    "    \n",
    "    # Show some examples of duplicates before removing\n",
    "    if duplicates.sum() > 0:\n",
    "        print(\"\\nExamples of potential spam:\")\n",
    "        spam_examples = combined_df[duplicates][['user_id', 'post_id', 'text', 'platform']].head()\n",
    "        print(spam_examples)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    combined_df = combined_df[~duplicates]\n",
    "    print(f\"\\nAfter removing duplicates: {len(combined_df)}\")\n",
    "    \n",
    "    # Also remove exact text duplicates (copy-paste spam)\n",
    "    text_duplicates = combined_df.duplicated(subset=['text'], keep='first')\n",
    "    print(f\"Found {text_duplicates.sum()} exact text duplicates\")\n",
    "    combined_df = combined_df[~text_duplicates]\n",
    "    print(f\"After removing text duplicates: {len(combined_df)}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Load the data\n",
    "df = load_and_clean_data()\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"\\nFinal dataset:\")\n",
    "    print(f\"Total comments: {len(df)}\")\n",
    "    print(f\"Platforms: {df['platform'].value_counts().to_dict()}\")\n",
    "    print(f\"Unique users: {df['user_id'].nunique()}\")\n",
    "    print(f\"Unique posts: {df['post_id'].nunique()}\")"
   ]
  }
  ,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"Applying advanced preprocessing...\")\n",
    "    \n",
    "    # Apply advanced cleaning\n",
    "    df['cleaned_text'] = df['text'].apply(advanced_clean_text)\n",
    "    \n",
    "    # Remove comments that became empty after cleaning\n",
    "    df = df[df['cleaned_text'].str.len() > 0]\n",
    "    print(f\"After advanced cleaning: {len(df)} comments\")\n",
    "    \n",
    "    # Show examples of preprocessing\n",
    "    print(\"\\nPreprocessing examples:\")\n",
    "    examples = df[['text', 'cleaned_text']].head(10)\n",
    "    for idx, row in examples.iterrows():\n",
    "        print(f\"Original: {row['text'][:100]}...\")\n",
    "        print(f\"Cleaned:  {row['cleaned_text'][:100]}...\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Apply stopword removal (optional - create both versions)\n",
    "    df['cleaned_no_stopwords'] = df['cleaned_text'].apply(remove_stopwords)\n",
    "    \n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(f\"Original text - Mean: {df['text'].str.len().mean():.1f}, Median: {df['text'].str.len().median():.1f}\")\n",
    "    print(f\"Cleaned text - Mean: {df['cleaned_text'].str.len().mean():.1f}, Median: {df['cleaned_text'].str.len().median():.1f}\")\n",
    "    print(f\"No stopwords - Mean: {df['cleaned_no_stopwords'].str.len().mean():.1f}, Median: {df['cleaned_no_stopwords'].str.len().median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_advanced(text):\n",
    "    \"\"\"Advanced sentiment analysis with better classification\"\"\"\n",
    "    if not text or text.strip() == '':\n",
    "        return 0, 0, 'neutral'\n",
    "    \n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    \n",
    "    # More nuanced sentiment classification\n",
    "    if polarity > 0.3:\n",
    "        sentiment = 'very_positive'\n",
    "    elif polarity > 0.1:\n",
    "        sentiment = 'positive'\n",
    "    elif polarity > -0.1:\n",
    "        sentiment = 'neutral'\n",
    "    elif polarity > -0.3:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'very_negative'\n",
    "    \n",
    "    return polarity, subjectivity, sentiment\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"Performing advanced sentiment analysis...\")\n",
    "    \n",
    "    # Analyze sentiment on cleaned text\n",
    "    sentiment_results = df['cleaned_text'].apply(analyze_sentiment_advanced)\n",
    "    df[['polarity', 'subjectivity', 'sentiment']] = pd.DataFrame(sentiment_results.tolist(), index=df.index)\n",
    "    \n",
    "    # Also analyze original text for comparison\n",
    "    original_sentiment = df['text'].apply(analyze_sentiment_advanced)\n",
    "    df[['original_polarity', 'original_subjectivity', 'original_sentiment']] = pd.DataFrame(original_sentiment.tolist(), index=df.index)\n",
    "    \n",
    "    print(\"Sentiment analysis completed!\")\n",
    "    \n",
    "    print(f\"\\nSentiment distribution (cleaned text):\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "    print(f\"\\nSentiment distribution (original text):\")\n",
    "    print(df['original_sentiment'].value_counts())\n",
    "    \n",
    "    print(f\"\\nComparison of preprocessing impact:\")\n",
    "    print(f\"Cleaned - Average polarity: {df['polarity'].mean():.3f}\")\n",
    "    print(f\"Original - Average polarity: {df['original_polarity'].mean():.3f}\")\n",
    "    \n",
    "    # Show examples where preprocessing changed sentiment\n",
    "    sentiment_changed = df[df['sentiment'] != df['original_sentiment']]\n",
    "    if len(sentiment_changed) > 0:\n",
    "        print(f\"\\nPreprocessing changed sentiment for {len(sentiment_changed)} comments:\")\n",
    "        for idx, row in sentiment_changed.head(5).iterrows():\n",
    "            print(f\"Text: {row['text'][:80]}...\")\n",
    "            print(f\"Original: {row['original_sentiment']} -> Cleaned: {row['sentiment']}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    \n",
    "    # 1. Sentiment distribution comparison\n",
    "    sentiment_comparison = pd.DataFrame({\n",
    "        'Original': df['original_sentiment'].value_counts(),\n",
    "        'Cleaned': df['sentiment'].value_counts()\n",
    "    }).fillna(0)\n",
    "    \n",
    "    sentiment_comparison.plot(kind='bar', ax=axes[0, 0], rot=45)\n",
    "    axes[0, 0].set_title('Sentiment Distribution: Original vs Cleaned Text')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Platform-wise sentiment\n",
    "    platform_sentiment = pd.crosstab(df['platform'], df['sentiment'])\n",
    "    platform_sentiment.plot(kind='bar', ax=axes[0, 1], rot=45)\n",
    "    axes[0, 1].set_title('Sentiment Distribution by Platform')\n",
    "    axes[0, 1].legend(title='Sentiment')\n",
    "    \n",
    "    # 3. Polarity distribution\n",
    "    axes[1, 0].hist([df['original_polarity'], df['polarity']], bins=30, alpha=0.7, \n",
    "                   label=['Original', 'Cleaned'], color=['red', 'blue'])\n",
    "    axes[1, 0].set_title('Polarity Distribution Comparison')\n",
    "    axes[1, 0].set_xlabel('Polarity Score')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Subjectivity distribution\n",
    "    axes[1, 1].hist([df['original_subjectivity'], df['subjectivity']], bins=30, alpha=0.7,\n",
    "                   label=['Original', 'Cleaned'], color=['orange', 'green'])\n",
    "    axes[1, 1].set_title('Subjectivity Distribution Comparison')\n",
    "    axes[1, 1].set_xlabel('Subjectivity Score')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # 5. Text length distribution\n",
    "    text_lengths = pd.DataFrame({\n",
    "        'Original': df['text'].str.len(),\n",
    "        'Cleaned': df['cleaned_text'].str.len(),\n",
    "        'No Stopwords': df['cleaned_no_stopwords'].str.len()\n",
    "    })\n",
    "    \n",
    "    text_lengths.boxplot(ax=axes[2, 0])\n",
    "    axes[2, 0].set_title('Text Length Distribution by Processing Stage')\n",
    "    axes[2, 0].set_ylabel('Character Count')\n",
    "    \n",
    "    # 6. Sentiment by text length\n",
    "    df['text_length_category'] = pd.cut(df['cleaned_text'].str.len(), \n",
    "                                       bins=[0, 20, 50, 100, float('inf')], \n",
    "                                       labels=['Very Short', 'Short', 'Medium', 'Long'])\n",
    "    \n",
    "    length_sentiment = pd.crosstab(df['text_length_category'], df['sentiment'])\n",
    "    length_sentiment.plot(kind='bar', ax=axes[2, 1], rot=45)\n",
    "    axes[2, 1].set_title('Sentiment by Text Length Category')\n",
    "    axes[2, 1].legend(title='Sentiment')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Preprocessed Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Save the processed dataset\n",
    "    output_path = '../data/processed_sentiment_data.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Processed data saved to: {output_path}\")\n",
    "    \n",
    "    # Save preprocessing functions for Flask app\n",
    "    preprocessing_functions = {\n",
    "        'slang_dict': SLANG_DICT,\n",
    "        'emoji_dict': EMOJI_DICT\n",
    "    }\n",
    "    \n",
    "    with open('../data/preprocessing_config.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(preprocessing_functions, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"Preprocessing configuration saved for Flask app\")\n",
    "    \n",
    "    # Create summary for Flask app\n",
    "    summary_stats = {\n",
    "        'total_comments': len(df),\n",
    "        'platform_distribution': df['platform'].value_counts().to_dict(),\n",
    "        'sentiment_distribution': df['sentiment'].value_counts().to_dict(),\n",
    "        'average_polarity': float(df['polarity'].mean()),\n",
    "        'average_subjectivity': float(df['subjectivity'].mean()),\n",
    "        'preprocessing_impact': {\n",
    "            'original_avg_polarity': float(df['original_polarity'].mean()),\n",
    "            'cleaned_avg_polarity': float(df['polarity'].mean()),\n",
    "            'sentiment_changes': len(df[df['sentiment'] != df['original_sentiment']])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('../data/model_summary.json', 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=2)\n",
    "    \n",
    "    print(\"Model summary saved for Flask app\")\n",
    "    print(\"\\nAdvanced preprocessing and analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
